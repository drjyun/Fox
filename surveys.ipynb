{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fox Survey Data\n",
    "- _Note_: `sub-074` was mislabeled in the pre-test as `sub-073`. For now, I'm manually changing that in the raw data before importing it into the notebook.\n",
    "- _Note_: `sub-001` was mislabeled in the pre-test. Changed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os, sys, re\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jin's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '../data/surveys'))\n",
    "today = dt.date.today()\n",
    "date = today \n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_filename = f'fox-post_{date.strftime(\"%B\")} {date.strftime(\"%-d\")}, {date.strftime(\"%Y\")}.csv'\n",
    "\n",
    "post = pd.read_csv(os.path.join(data_dir, post_filename), skiprows=[1,2], encoding='utf-8-sig', parse_dates=[1,2], dayfirst=False)\n",
    "\n",
    "conditions = {\n",
    "    'LEGO': 1, 'BIG MOOD': 2, 'American Pickers': 3,\n",
    "    'CNN News': 4, 'Continental': 5, 'Fox News': 6,\n",
    "    'NFL': 7, 'YouTube TV': 8, 'YouTube - TV': 8,\n",
    "    'Smartphone A - YouTube first': 9, 'Smartphone B - Facebook first': 10,\n",
    "    'Control - TV': 11, 'Control - Smartphone': 12\n",
    "}\n",
    "conditions_normalized = {k.lower(): v for k, v in conditions.items()}\n",
    "\n",
    "post['condition_normalized'] = post['condition'].str.strip().str.lower()\n",
    "post['Cond'] = post['condition_normalized'].map(conditions_normalized)\n",
    "post['Cond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = post[['ResponseId', 'Cond', 'condition', 'gender', 'age', 'glasses', 'race', 'education', 'income', 'politics']].copy()\n",
    "demos['F'] = np.where(demos['gender'] == 2, 1, 0)\n",
    "demos['M'] = np.where(demos['gender'] == 1, 1, 0)\n",
    "\n",
    "demo_sum = demos.groupby('Cond').agg({\n",
    "    'condition': ['size'],\n",
    "    'age': ['min', 'mean', 'max', 'std'],\n",
    "    'F': ['sum'],\n",
    "    'M': ['sum']\n",
    "}).round(2)\n",
    "\n",
    "demo_sum_flat = pd.DataFrame(demo_sum.to_records())\n",
    "demo_sum_flat.columns = ['Cond', 'Participants', 'Age_Min', 'Age_Mean', 'Age_Max', 'Age_Std', 'Female_Count', 'Male_Count']\n",
    "\n",
    "conditions_swap = {v: k for k, v in conditions.items()}\n",
    "demo_sum_flat['Cond'] = demo_sum_flat['Cond'].map(conditions_swap)\n",
    "\n",
    "# Filter out conditions\n",
    "demo_sum_filtered = demo_sum_flat[(demo_sum_flat[\"Cond\"] != 'Control - Smartphone') & (demo_sum_flat[\"Cond\"] != 'YouTube - TV')]\n",
    "\n",
    "sns.set(style='whitegrid', context='talk')\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Participants per Condition\n",
    "sns.barplot(\n",
    "    x='Participants',\n",
    "    y='Cond',\n",
    "    data=demo_sum_filtered,\n",
    "    palette='coolwarm',\n",
    "    orient='h',\n",
    "    ax=axs[0]\n",
    ")\n",
    "axs[0].set_xlabel('# of Participants', fontsize=18)\n",
    "axs[0].set_ylabel('')\n",
    "axs[0].set_title('Participants per Condition', fontsize=24)\n",
    "axs[0].set_xlim(right=max(demo_sum_filtered['Participants']) + 2)\n",
    "\n",
    "# Mean Age per Condition \n",
    "sns.barplot(\n",
    "    x='Age_Mean',\n",
    "    y='Cond',\n",
    "    data=demo_sum_filtered,\n",
    "    palette='coolwarm',\n",
    "    orient='h',\n",
    "    ax=axs[1],\n",
    "    xerr=demo_sum_filtered['Age_Std']\n",
    ")\n",
    "axs[1].set_xlabel('Mean Age', fontsize=18)\n",
    "axs[1].set_ylabel('')\n",
    "axs[1].set_title('Mean Age per Condition', fontsize=24)\n",
    "axs[1].set_xlim(right=max(demo_sum_filtered['Age_Mean']) + 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sum_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of participants by gender\n",
    "demos = post[['ResponseId', 'gender']].copy()\n",
    "gender_counts = demos['gender'].value_counts()\n",
    "print(gender_counts)\n",
    "\n",
    "gender_labels = {1: 'Male', 2: 'Female', 3: 'Non-binary / third gender'}\n",
    "gender_counts.index = gender_counts.index.map(gender_labels)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(\n",
    "    gender_counts,\n",
    "    # labels=gender_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=sns.color_palette('coolwarm', len(gender_counts)),\n",
    "    startangle=140\n",
    ")\n",
    "plt.title('Gender Proportion')\n",
    "plt.axis('equal')  \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_conditions = ['LEGO', 'BIG MOOD', 'American Pickers', 'CNN News', 'Continental', 'Fox News', 'NFL']\n",
    "social_media_conditions = ['Smartphone A - YouTube first', 'Smartphone B - Facebook first']\n",
    "\n",
    "# Count recall data for each TV condition\n",
    "tv_recall_counts = post[post['condition'].isin(tv_conditions)]['recall'].value_counts()\n",
    "\n",
    "# Count recall data for each digital condition\n",
    "social_media_recall_counts = post[post['condition'].isin(social_media_conditions)]['recall'].value_counts()\n",
    "tv_recall_counts, social_media_recall_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_brands(recall_data):\n",
    "    brand_counts = Counter()\n",
    "    for response in recall_data.dropna():\n",
    "        brands = response.split(',')\n",
    "        for brand in brands:\n",
    "            brand_counts[brand.strip().lower()] += 1\n",
    "    return brand_counts\n",
    "\n",
    "# Count brands for TV conditions\n",
    "tv_brand_counts = count_brands(post[post['condition'].isin(tv_conditions)]['recall'])\n",
    "\n",
    "# Count brands for Social Media conditions\n",
    "social_media_brand_counts = count_brands(post[post['condition'].isin(social_media_conditions)]['recall'])\n",
    "\n",
    "tv_brand_counts, social_media_brand_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization_map = {\n",
    "    'red bull': ['red bull', 'redbull', 'red bull (juneberry)', 'red bulls', 'redbulls'],\n",
    "    'paramount+': ['paramount+', 'paramount plus', 'paramount', 'paramoun+'],\n",
    "    'burger king': ['burger king', 'burgerking', 'burger-king', 'burger kings', 'buger king'],\n",
    "    'rakuten': ['rakuten', 'rakutu', 'rakutenn'],\n",
    "    'chevy': ['chevy', 'chevrolet', 'chevorlet', 'chevvy', 'chev'],\n",
    "    'samsung': ['samsung', 'sam sung', 'samsng', 'sumsung', 'sam-sung'],\n",
    "    'tracfone': ['tracfone', 'trac-fone', 'tracfones'],\n",
    "    'mounjaro': ['Mojourna', 'majaran', 'majorn', 'majoran', 'majoarn', 'majouran', 'Majaron', 'Manjara']\n",
    "}\n",
    "\n",
    "def standardize_recall_with_nan(recall_responses, standardization_map):\n",
    "    standardized_responses = []\n",
    "    for response in recall_responses:\n",
    "        if pd.notna(response):\n",
    "            standardized_response = []\n",
    "            brands = response.split(',')\n",
    "            for brand in brands:\n",
    "                brand_cleaned = brand.strip().lower()\n",
    "                standardized = False\n",
    "                for standard_brand, variations in standardization_map.items():\n",
    "                    if brand_cleaned in variations:\n",
    "                        standardized_response.append(standard_brand)\n",
    "                        standardized = True\n",
    "                        break\n",
    "                if not standardized:\n",
    "                    standardized_response.append(brand_cleaned)  # Keep as is if no match found\n",
    "            standardized_responses.append(','.join(standardized_response))\n",
    "        else:\n",
    "            standardized_responses.append(response)\n",
    "    return standardized_responses\n",
    "\n",
    "post['standardized_recall'] = standardize_recall_with_nan(post['recall'], standardization_map)\n",
    "post[['recall', 'standardized_recall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_ads_list = list(standardization_map.keys())\n",
    "\n",
    "# Function to create 0/1 values for recall of each ad\n",
    "def create_recall_matrix_standardized(recall_data, ads):\n",
    "    recall_matrix = pd.DataFrame(index=recall_data.index, columns=ads, data=0)\n",
    "    for idx, row in recall_data.iterrows():\n",
    "        if pd.notna(row['standardized_recall']):\n",
    "            recalled_ads = [ad.strip().lower() for ad in row['standardized_recall'].split(',')]\n",
    "            for ad in ads:\n",
    "                if ad.lower() in recalled_ads:\n",
    "                    recall_matrix.loc[idx, ad] = 1\n",
    "    return recall_matrix\n",
    "\n",
    "recall_matrix_standardized = create_recall_matrix_standardized(post[['id', 'condition', 'standardized_recall']], standardized_ads_list)\n",
    "recall_matrix_standardized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_recall_data_standardized = pd.concat([post[['id', 'condition']], recall_matrix_standardized], axis=1)\n",
    "\n",
    "# Reshape the dataset to a long format\n",
    "nested_recall = nested_recall_data_standardized.melt(id_vars=['id', 'condition'], \n",
    "                                                     value_vars=standardized_ads_list, \n",
    "                                                     var_name='ad', \n",
    "                                                     value_name='recall')\n",
    "\n",
    "nested_recall = nested_recall[['id', 'condition', 'ad', 'recall']]\n",
    "nested_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'sub' and 'condition' to repeat subject ID for each condition\n",
    "nested_recall = nested_recall.groupby(['id', 'condition', 'ad']).agg(recall=('recall', 'sum')).reset_index()\n",
    "\n",
    "nested_recall = nested_recall[['id', 'condition', 'ad', 'recall']]\n",
    "nested_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse the subjects within each condition\n",
    "collapsed_recall = nested_recall.groupby(['condition', 'ad']).agg(\n",
    "    recall_sum=('recall', 'sum'),\n",
    "    recall_count=('recall', 'count')\n",
    ").reset_index()\n",
    "\n",
    "collapsed_recall['recall_rate'] = collapsed_recall['recall_sum'] / collapsed_recall['recall_count']\n",
    "collapsed_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_conditions = ['LEGO', 'BIG MOOD', 'American Pickers', 'CNN News', 'Continental', 'Fox News', 'NFL']\n",
    "digital_conditions = ['Smartphone A - YouTube first', 'Smartphone B - Facebook first']\n",
    "\n",
    "tv_recall_data = nested_recall[nested_recall['condition'].isin(tv_conditions)]\n",
    "digital_recall_data = nested_recall[nested_recall['condition'].isin(digital_conditions)]\n",
    "\n",
    "# Calculate the recall rates for each ad in TV conditions\n",
    "tv_recall_rates = tv_recall_data.groupby('ad')['recall'].mean().reset_index()\n",
    "tv_recall_rates.columns = ['ad', 'TV_Recall_Rate']\n",
    "\n",
    "# Calculate the recall rates for each ad in digital conditions\n",
    "digital_recall_rates = digital_recall_data.groupby('ad')['recall'].mean().reset_index()\n",
    "digital_recall_rates.columns = ['ad', 'Digital_Recall_Rate']\n",
    "\n",
    "recall_comparison = pd.merge(tv_recall_rates, digital_recall_rates, on='ad')\n",
    "recall_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_results = []\n",
    "for ad in standardized_ads_list:\n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(\n",
    "        nested_recall[nested_recall['ad'] == ad]['condition'].isin(tv_conditions),\n",
    "        nested_recall[nested_recall['ad'] == ad]['recall']\n",
    "    )\n",
    "    \n",
    "    # Perform Chi-square test\n",
    "    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "    chi2_results.append((ad, chi2_stat, p_val))\n",
    "\n",
    "chi2_results_df = pd.DataFrame(chi2_results, columns=['ad', 'Chi2-Statistic', 'P-Value'])\n",
    "chi2_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liz' code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/r7mkx_mj2d73zrgl4w0848ww0000gq/T/ipykernel_93869/2851332371.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pre = pd.read_csv(os.path.join(data_dir, pre_filename), skiprows=[1,2], encoding='utf-8-sig', parse_dates=[1,2], dayfirst=False)\n",
      "/var/folders/4z/r7mkx_mj2d73zrgl4w0848ww0000gq/T/ipykernel_93869/2851332371.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  post = pd.read_csv(os.path.join(data_dir, post_filename), skiprows=[1,2], encoding='utf-8-sig', parse_dates=[1,2], dayfirst=False)\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), '../data/surveys'))\n",
    "today = dt.date.today()\n",
    "date = today #.replace(day=today.day-1) #yesterday's data, change as needed\n",
    "print(date)\n",
    "# as long as you remove the time from the datasheet (##.##.csv), you don't need to manually change anything\n",
    "pre_filename = f'fox-pre_July+17.csv' \n",
    "post_filename = f'fox-post_July+17.csv' \n",
    "\n",
    "pre = pd.read_csv(os.path.join(data_dir, pre_filename), skiprows=[1,2], encoding='utf-8-sig', parse_dates=[1,2], dayfirst=False)\n",
    "post = pd.read_csv(os.path.join(data_dir, post_filename), skiprows=[1,2], encoding='utf-8-sig', parse_dates=[1,2], dayfirst=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Control - Smartphone', nan, 'LEGO', 'BIG MOOD',\n",
       "       'American Pickers', 'CNN News', 'Continental', 'Fox News', 'NFL',\n",
       "       'Control - TV', 'Smartphone A - YouTube first',\n",
       "       'Smartphone B - Facebook first', 'Youtube - TV'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.condition.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = {'LEGO':1, 'BIG MOOD':2, 'American Pickers':3, \n",
    "              'CNN News':4, 'Continental':5, 'Fox News':6, \n",
    "              'NFL':7, 'YouTube TV':8, 'Smartphone A':12,\n",
    "              'Smartphone B':10, 'Control TV':11, 'Control Smartphone':13}\n",
    "conditions_swap = {v: k for k, v in conditions.items()}\n",
    "\n",
    "post['Cond'] = post['condition'].map(conditions_swap)\n",
    "pre['Cond'] = pre['condition'].map(conditions_swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = post[['id', 'Cond', 'condition', 'gender', 'age', 'glasses', 'race', 'education', 'income', 'politics']].copy()\n",
    "demos['F'] = np.where(demos['gender']==2, 1,0)\n",
    "\n",
    "demo_sum = demos.groupby('Cond').agg({\n",
    "    'condition': ['size'],\n",
    "    'age': ['min', 'mean', 'max'],\n",
    "    'F': ['sum']\n",
    "}).round(2)\n",
    "\n",
    "demo_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sum_flat = pd.DataFrame(demo_sum.to_records())\n",
    "#plt.bar(demo_sum_flat.iloc[:,1], demo_sum_flat['Cond'])\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.xlabel('# of Participants')\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "hbars = ax.barh(demo_sum_flat.loc[demo_sum_flat[\"Cond\"]!='Control Smartphone', \"Cond\"], demo_sum_flat.iloc[-4,1])\n",
    "ax.set_xlabel('# of Participants')\n",
    "ax.set_xlim(right=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejectors\n",
    "A lot of rejectors in this group for Fox Sports and Fox News. Some still made their way into the condition they're a rejector of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejectors = pre[['sub_id', 'Cond', 'condition', 'rejector_FOX-Prime', 'rejector_FOX-Sports', 'rejector_FOX-News']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejectors[['rejector_FOX-Prime', 'rejector_FOX-Sports', 'rejector_FOX-News']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_miss = (rejectors['rejector_FOX-Prime']==True) & (rejectors['Cond']=='LEGO')\n",
    "sports_miss = (rejectors['rejector_FOX-Sports']==True) & (rejectors['Cond']=='NFL')\n",
    "news_miss = (rejectors['rejector_FOX-News']==True) & (rejectors['Cond']=='Fox News')\n",
    "\n",
    "rejectors['miss'] = np.where(prime_miss|sports_miss|news_miss,True,False)\n",
    "rejectors[(rejectors['miss']==True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Willingness To Pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp = post.filter(regex='id|Cond|wtp')\n",
    "wtp_t = post.filter(regex='id|Cond|wtp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to convert strings with non-numeric characters to float\n",
    "def convert_to_number(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove non-numeric except period and minus\n",
    "        cleaned_value = re.sub(r'[^\\d.-]', '', value)\n",
    "        # Convert to float if possible, else 0\n",
    "        try:\n",
    "            return float(cleaned_value)\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    else:\n",
    "        return float(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to clean\n",
    "wtp_columns = [col for col in wtp.columns if 'wtp' in col]\n",
    "\n",
    "# Apply the conversion\n",
    "for col in wtp_columns:\n",
    "    wtp[col] = wtp[col].apply(convert_to_number)\n",
    "\n",
    "# Show the cleaned data and check types\n",
    "wtp[wtp_columns].head(), wtp[wtp_columns].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_mapping = {\n",
    "    '1_wtp_1': 'Tracfone_wtp',\n",
    "    '2_wtp_1': 'Rakuten_wtp',\n",
    "    '3_wtp_1': 'Paramount_wtp',\n",
    "    '4_wtp_1': 'Samsung_wtp',\n",
    "    '5_wtp_1': 'BurgerKing_wtp',\n",
    "    '6_wtp_1': 'RedBull_wtp',\n",
    "    '7_wtp_1': 'Chevrolet_wtp',\n",
    "    '8_wtp_1': 'Lilly_wtp'\n",
    "}\n",
    "\n",
    "wtp.rename(columns=column_name_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_filename = f'fox-wtp_{date.strftime(\"%B\")} {date.strftime(\"%-d\")}, {date.strftime(\"%Y\")}.csv' \n",
    "wtp.to_csv(os.path.join(data_dir, wtp_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Post Persuasion\n",
    "??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pre = pre.filter(regex='id|top')\n",
    "top_post = post.filter(regex='id|condition|top')\n",
    "\n",
    "# top_pre.to_csv(os.path.join(data_dir, 'top_pre.csv'))\n",
    "# top_post.to_csv(os.path.join(data_dir, 'top_post.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/r7mkx_mj2d73zrgl4w0848ww0000gq/T/ipykernel_93869/2554744847.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_pre['numeric_id'] = top_pre['sub_id'].str.extract('(\\d+)')\n",
      "/var/folders/4z/r7mkx_mj2d73zrgl4w0848ww0000gq/T/ipykernel_93869/2554744847.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_post['numeric_id'] = top_post['id'].str.extract('(\\d+)')\n"
     ]
    }
   ],
   "source": [
    "# Extract digits from the ID column to create a consistent key for merging\n",
    "top_pre['numeric_id'] = top_pre['sub_id'].str.extract('(\\d+)')\n",
    "top_post['numeric_id'] = top_post['id'].str.extract('(\\d+)')\n",
    "\n",
    "# Merge the original pre and post survey data on the numeric ID column\n",
    "merged_data_numeric_id = pd.merge(top_pre, top_post, on='numeric_id', suffixes=('_pre', '_post'))\n",
    "\n",
    "merged_data_numeric_id.to_csv(os.path.join(data_dir, 'pre_post.csv'))\n",
    "\n",
    "# Display the merged data and its dimensions\n",
    "# merged_data_numeric_id.head(), merged_data_numeric_id.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply the updated check for each brand and concatenate the results\n",
    "updated_results_3 = pd.DataFrame()\n",
    "for brand, alternatives in brand_alternatives.items():\n",
    "    updated_results_3 = pd.concat([updated_results_3, check_brand_mention_updated(merged_data_numeric_id.copy(), brand, alternatives, id_col='numeric_id')], axis=1)\n",
    "\n",
    "# Remove duplicated columns if any due to repeated merging\n",
    "updated_results_3 = updated_results_3.loc[:,~updated_results_3.columns.duplicated()].copy()\n",
    "\n",
    "# Save or display results as needed\n",
    "# updated_results.to_csv('updated_top_of_mind.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_brand_mention_updated(df, brand_base, alternative_names=None, id_col=None):\n",
    "    pre_col = f'top_pre_{brand_base}'\n",
    "    post_col = f'top_post_{brand_base}'\n",
    "    brand = brand_base.split('_')[0]  # Assumes brand name is the first part of the column label\n",
    "    \n",
    "    # Enhance handling of alternative names or multi-word brands\n",
    "    if alternative_names:\n",
    "        # Create a regex pattern that matches any of the alternative names\n",
    "        check_names = '|'.join([f\"\\\\b{name}\\\\b\" for name in alternative_names])  # \\b is a word boundary in regex\n",
    "    else:\n",
    "        check_names = f\"\\\\b{brand.replace('_', ' ')}\\\\b\"  # Replace underscore with space for multi-word brands\n",
    "\n",
    "    # Check if the brand name appears in the pre and post columns (ignoring case)\n",
    "    df[f'{brand}_pre'] = df[pre_col].str.contains(check_names, na=False, case=False, regex=True)\n",
    "    df[f'{brand}_post'] = df[post_col].str.contains(check_names, na=False, case=False, regex=True)\n",
    "    df[f'{brand}_post_persuasion'] = (~df[f'{brand}_pre']) & (df[f'{brand}_post'])\n",
    "\n",
    "    return df[[id_col, pre_col, post_col, f'{brand}_pre', f'{brand}_post', f'{brand}_post_persuasion']]\n",
    "\n",
    "# Define alternative names for brands where necessary\n",
    "brand_alternatives = {\n",
    "    'tracfone': ['tracfone'],\n",
    "    'rakuten': ['rakuten'],\n",
    "    'paramount': ['paramount'],\n",
    "    'samsung': ['samsung'],\n",
    "    'burgerking': ['burger king'],\n",
    "    'redbull': ['red bull'],\n",
    "    'chevrolet': ['chevrolet'],\n",
    "    'lilly': ['lilly', 'mounjaro']\n",
    "}\n",
    "\n",
    "# Apply the updated check for each brand and concatenate the results\n",
    "updated_results = pd.DataFrame()\n",
    "for brand, alternatives in brand_alternatives.items():\n",
    "    updated_results = pd.concat([updated_results, check_brand_mention_updated(merged_data_numeric_id.copy(), brand, alternatives, id_col='numeric_id')], axis=1)\n",
    "\n",
    "updated_results = updated_results.loc[:,~updated_results.columns.duplicated()].copy()\n",
    "updated_results =  updated_results.merge(top_post[['numeric_id', 'condition']], on='numeric_id', how='left')\n",
    "#updated_results.head()\n",
    "# updated_results.to_csv(os.path.join(data_dir, 'top_of_mind.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Control - Smartphone', 'LEGO', 'BIG MOOD', 'American Pickers',\n",
       "       'CNN News', 'Fox News', 'NFL', 'Control - TV', 'Continental',\n",
       "       'Smartphone A - YouTube first', 'Smartphone B - Facebook first',\n",
       "       'YouTube - TV', nan], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_results.condition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_conditions = ['LEGO', 'BIG MOOD', 'American Pickers', 'CNN News', 'Continental', 'Fox News', 'NFL', 'Control - TV']\n",
    "social_media_conditions = ['Smartphone A - YouTube first', 'Smartphone B - Facebook first']\n",
    "\n",
    "# Columns to analyze for post persuasion\n",
    "post_persuasion_columns = [col for col in updated_results.columns if 'post_persuasion' in col]\n",
    "\n",
    "# List to store results for DataFrame conversion\n",
    "results_list = []\n",
    "\n",
    "# Function to calculate post persuasion rate and store results\n",
    "def calculate_post_persuasion_rates(df_subset, condition):\n",
    "    for column in post_persuasion_columns:\n",
    "        # Calculate the mean of True values which represent the proportion\n",
    "        rate = df_subset[column].mean()\n",
    "        # Store each result as a dictionary in the results list\n",
    "        results_list.append({\n",
    "            'Condition': condition,\n",
    "            'Brand': column.replace('_post_persuasion', ''),\n",
    "            'Post_Persuasion_Rate': rate\n",
    "        })\n",
    "\n",
    "# Analyze TV conditions\n",
    "for condition in tv_conditions:\n",
    "    condition_data = updated_results[updated_results['condition'] == condition]  # Adjust 'condition' to your dataset's column name\n",
    "    calculate_post_persuasion_rates(condition_data, condition)\n",
    "\n",
    "# Analyze Social Media conditions\n",
    "for condition in social_media_conditions:\n",
    "    condition_data = updated_results[updated_results['condition'] == condition]  # Adjust 'condition' to your dataset's column name\n",
    "    calculate_post_persuasion_rates(condition_data, condition)\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df.to_csv(os.path.join(data_dir,'post_persuasion_rates.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
